{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 700        # Number of input neurons (same as num_units)\n",
    "num_classes = 20        # Adjust based on the dataset's number of classes\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "time_window = 2         # Time steps for temporal processing\n",
    "batch_size = 4          # Define batch size here\n",
    "beta1 = 0.8             # Decay factor for RLeaky layer 1\n",
    "beta2 = 0.89            # Decay factor for RLeaky layer 2\n",
    "num_units = 700         # Number of input units (same as input_size)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import gzip, shutil\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Download and extract files\n",
    "def download_and_extract_shd():\n",
    "    url = \"https://zenkelab.org/datasets/shd_train.h5.gz\"\n",
    "    gz_file_path = os.path.join(cache_dir, \"shd_train.h5.gz\")\n",
    "    hdf5_file_path = gz_file_path[:-3]\n",
    "\n",
    "    if not os.path.exists(hdf5_file_path):\n",
    "        print(\"Downloading SHD dataset...\")\n",
    "        urllib.request.urlretrieve(url, gz_file_path)\n",
    "        \n",
    "        # Decompress .gz file\n",
    "        print(f\"Decompressing {gz_file_path}...\")\n",
    "        with gzip.open(gz_file_path, 'rb') as f_in, open(hdf5_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Decompressed to {hdf5_file_path}\")\n",
    "    else:\n",
    "        print(f\"{hdf5_file_path} already exists.\")\n",
    "    \n",
    "    return hdf5_file_path\n",
    "\n",
    "# Download and extract the training set\n",
    "shd_train_file = download_and_extract_shd()\n",
    "shd_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def explore_h5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # Recursive function to explore groups and datasets\n",
    "        def print_group(name, obj):\n",
    "            print(f\"Group: {name}\")\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                for key in obj.keys():\n",
    "                    print_group(f\"{name}/{key}\", obj[key])\n",
    "            else:\n",
    "                print(f\"  Dataset: {name}, Shape: {obj.shape}, Dtype: {obj.dtype}\")\n",
    "\n",
    "        print_group('/', file)\n",
    "        \n",
    "explore_h5_file(shd_train_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Function to load spikes and labels from the SHD dataset\n",
    "def load_spikes_and_labels(hdf5_file_path):\n",
    "    with h5py.File(hdf5_file_path, 'r') as f:\n",
    "        # Extract the labels\n",
    "        labels = np.array(f['/labels'])\n",
    "        \n",
    "        # Extract the spike units (neurons that fired)\n",
    "        spike_units = np.array(f['/spikes/units'], dtype=object)  # Shape: (8156,)\n",
    "        \n",
    "        # Extract the spike times (optional if you want to analyze timings as well)\n",
    "        spike_times = np.array(f['/spikes/times'], dtype=object)\n",
    "        \n",
    "    return spike_units, spike_times, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "spike_units, spike_times, labels = load_spikes_and_labels(shd_train_file)\n",
    "\n",
    "\n",
    "spike_units_list = [torch.tensor(units, dtype=torch.float32) for units in spike_units]\n",
    "spike_times_list = [torch.tensor(times, dtype=torch.float32) for times in spike_times]\n",
    "\n",
    "\n",
    "# Limit the number of samples to process (e.g., 500 samples)\n",
    "num_samples = 500\n",
    "spike_units_list = spike_units_list[:num_samples]\n",
    "spike_times_list = spike_times_list[:num_samples]\n",
    "labels = labels[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.spikegen as spikegen\n",
    "\n",
    "# Function to convert spike times and units into tensor format for SLSTM input\n",
    "def generate_spike_tensor(spike_units, spike_times, num_units, time_window):\n",
    "    \"\"\"\n",
    "    Convert spike units and spike times to spike train tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    - spike_units: tensor of spike units (neuron IDs)\n",
    "    - spike_times: tensor of spike times (in ms)\n",
    "    - num_units: number of possible units (e.g., 700 in SHD)\n",
    "    - time_window: total time of the spike train (in ms)\n",
    "    \n",
    "    Returns:\n",
    "    - A spike tensor of shape [time_window, num_units]\n",
    "    \"\"\"\n",
    "    spike_tensor = torch.zeros((time_window, num_units))\n",
    "\n",
    "    for unit, time in zip(spike_units, spike_times):\n",
    "        # Time is rounded to the nearest ms\"\n",
    "        spike_time = int(time.item())\n",
    "        if spike_time < time_window:\n",
    "            spike_tensor[spike_time, int(unit.item())] = 1\n",
    "\n",
    "    return spike_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_list = [generate_spike_tensor(spike_units_list[i], spike_times_list[i], num_units, time_window) for i in range(num_samples)]\n",
    "spike_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor = [torch.tensor(i, dtype=torch.long).unsqueeze(0) for i in labels]\n",
    "labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import snntorch as snn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Define the Spiking Recurrent Neural Network (SRNN) Model\n",
    "class SRNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, time_window):\n",
    "        super(SRNN, self).__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.rlif1 = snn.RLeaky(beta=beta1, all_to_all=True, linear_features=512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.rlif2 = snn.RLeaky(beta=beta2, all_to_all=True, linear_features=num_classes)\n",
    "\n",
    "        self.time_window = time_window\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize spikes and membrane potential for both layers\n",
    "        spk1, mem1 = torch.ones(512, device=x.device) * 0.2, torch.ones(512, device=x.device) * 0.1\n",
    "        spk2, mem2 = torch.ones(num_classes, device=x.device) * 0.2, torch.ones(num_classes, device=x.device) * 0.1\n",
    "\n",
    "        spk2_list, mem2_list = [], []\n",
    "\n",
    "        # Iterate over the time window\n",
    "        for step in range(self.time_window):\n",
    "            ip = x[:, step, :]  # Access time step for entire batch; Shape: (batch_size, num_units)\n",
    "\n",
    "            # Layer 1: Fully connected + RLeaky\n",
    "            out = self.fc1(ip)\n",
    "            spk1, mem1 = self.rlif1(out, spk1, mem1)\n",
    "\n",
    "            # Layer 2: Fully connected + RLeaky\n",
    "            out = self.fc2(spk1)\n",
    "            spk2, mem2 = self.rlif2(out, spk2, mem2)\n",
    "\n",
    "            # Append results for the time step\n",
    "            spk2_list.append(spk2)\n",
    "\n",
    "        # Stack results from each time step to form tensors\n",
    "        spk2_tensor = torch.stack(spk2_list, dim=1)  # Shape: (batch_size, time_window, num_classes)\n",
    "\n",
    "        return spk2_tensor\n",
    "\n",
    "\n",
    "# Convert to torch tensors and use DataLoader for batching\n",
    "spike_tensor = torch.stack(spike_list)  # Shape: (100, time_window, num_units)\n",
    "labels_tensor = torch.cat(labels_tensor)        # Shape: (100,)\n",
    "\n",
    "print(spike_tensor.size())\n",
    "print(labels_tensor.size())\n",
    "\n",
    "# Create a TensorDataset and DataLoader for batching\n",
    "dataset = TensorDataset(spike_tensor, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SRNN(input_size, num_classes, time_window).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (spike_batch, label_batch) in enumerate(data_loader):\n",
    "        # Move batch to device (GPU or CPU)\n",
    "        spike_batch = spike_batch.to(device)  # Shape: (batch_size, time_window, num_units)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(spike_batch)  # Output shape: (batch_size, time_window, num_classes)\n",
    "        \n",
    "        # To compute loss, flatten the outputs and labels\n",
    "        outputs = outputs.mean(dim=1)  # Aggregating over the time dimension\n",
    "        loss = criterion(outputs, label_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress for every batch\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
